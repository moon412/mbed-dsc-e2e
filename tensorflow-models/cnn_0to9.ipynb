{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This script is based on:\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util as gu\n",
    "from tensorflow.python.framework.graph_util import remove_training_nodes\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras.preprocessing.image' from '/Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/keras/preprocessing/image/__init__.py'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.preprocessing.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MORE IMAGES VIA DATA AUGMENTATION\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.10,  \n",
    "        width_shift_range=0.1, \n",
    "        height_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[9,].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tmp, label_tmp = datagen.flow(mnist.train.images[9,].reshape(1, 28, 28, 1), \n",
    "                                    mnist.train.labels[9,].reshape(1, 10)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tmp.reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size, num_channels, num_classes = 28, 1, 10\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev, wd):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "def bias_variable(shape, val):\n",
    "    initial = tf.constant(val, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32], stddev=0.1, wd=None)\n",
    "b_conv1 = bias_variable([32], 0.1)\n",
    "conv1 = tf.nn.conv2d(x, W_conv1, [1, 1, 1, 1], padding='SAME', name=\"conv1\")\n",
    "h_conv1 = tf.nn.relu(tf.add(conv1, b_conv1), name=\"h_conv1\")\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "drop1 = tf.nn.dropout(h_pool1, keep_prob=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 32], stddev=0.1, wd=None)\n",
    "b_conv2 = bias_variable([32], 0.1)\n",
    "conv2 = tf.nn.conv2d(drop1, W_conv2, [1, 1, 1, 1], padding='SAME', name=\"conv2\")\n",
    "h_conv2 = tf.nn.relu(tf.add(conv2, b_conv2), name=\"h_conv2\")\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "drop2 = tf.nn.dropout(h_pool2, keep_prob=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv3 = weight_variable([5, 5, 32, 32], stddev=0.1, wd=None)\n",
    "b_conv3 = bias_variable([32], 0.1)\n",
    "conv3 = tf.nn.conv2d(drop2, W_conv3, [1, 1, 1, 1], padding='SAME', name=\"conv3\")\n",
    "h_conv3 = tf.nn.relu(tf.add(conv3, b_conv3), name=\"h_conv3\")\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "#drop3 = tf.nn.dropout(h_pool3, keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 4 * 4 * 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([4 * 4 * 32, 128], stddev=0.1, wd=None)\n",
    "b_fc1 = bias_variable([128], 0.1)\n",
    "h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_pool3_flat, W_fc1), b_fc1), name=\"fc1\")\n",
    "drop4 = tf.nn.dropout(h_fc1, keep_prob=1.0)\n",
    "\n",
    "W_fc2 = weight_variable([128, 10], stddev=0.1, wd=None)\n",
    "b_fc2 = bias_variable([10], 0.1)\n",
    "logits = tf.add(tf.matmul(drop4, W_fc2), b_fc2, name=\"logits\")\n",
    "y_pred = tf.argmax(logits, axis=1, name=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, \n",
    "                                                               logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"cross_entropy_loss\")\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss, name=\"train_step\")\n",
    "  \n",
    "# Here we specify the output as \"Prediction/y_pred\", this will be important later\n",
    "with tf.name_scope(\"Prediction\"): \n",
    "    correct_prediction = tf.equal(y_pred, \n",
    "                                  tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 50\n",
    "n_batches = int(mnist.train.num_examples / batch_size)\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 4.22769, training accuracy: 0.0989818\n",
      "Epoch 0, validation loss: 4.26632, validation accuracy 0.0958\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                      feed_dict={x: mnist.train.images.reshape(mnist.train.num_examples, img_size, img_size, num_channels), \n",
    "                                                 y_: mnist.train.labels})\n",
    "print('Epoch %d, training loss: %g, training accuracy: %g' % (0, train_loss, train_accuracy))\n",
    "val_loss, val_accuracy = sess.run([loss, accuracy], \n",
    "                                  feed_dict={x: mnist.validation.images.reshape(mnist.validation.num_examples, img_size, img_size, num_channels),\n",
    "                                             y_: mnist.validation.labels})\n",
    "print('Epoch %d, validation loss: %g, validation accuracy %g' % (0, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    for j in range(n_batches):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={x: batch_images.reshape(batch_size, img_size, img_size, num_channels), \n",
    "                                        y_: batch_labels})\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                              feed_dict={x: mnist.train.images.reshape(mnist.train.num_examples, img_size, img_size, num_channels), \n",
    "                                                         y_: mnist.train.labels})\n",
    "        print('Epoch %d, training loss: %g, training accuracy: %g' % (i, train_loss, train_accuracy))\n",
    "        val_loss, val_accuracy = sess.run([loss, accuracy], \n",
    "                                          feed_dict={x: mnist.validation.images.reshape(mnist.validation.num_examples, img_size, img_size, num_channels),\n",
    "                                                     y_: mnist.validation.labels})\n",
    "        print('Epoch %d, validation loss: %g, validation accuracy %g' % (i, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9898\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy %g' % sess.run(accuracy, \n",
    "                                    feed_dict={x: mnist.validation.images.reshape(mnist.validation.num_examples, img_size, img_size, num_channels), \n",
    "                                               y_: mnist.validation.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9914\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy %g' % sess.run(accuracy, \n",
    "                                    feed_dict={x: mnist.test.images.reshape(mnist.test.num_examples, img_size, img_size, num_channels), \n",
    "                                               y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y_pred']\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, \"./chkps/mnist_cnn_small_dropout08_lr04\")\n",
    "out_nodes = [y_pred.op.name]\n",
    "print(out_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_def = remove_training_nodes(sess.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "sub_graph_def = gu.convert_variables_to_constants(sess, sub_graph_def, out_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written graph to: ./mnist_cnn_small_dropout08_lr04/mnist_cnn.pb\n"
     ]
    }
   ],
   "source": [
    "graph_path = tf.train.write_graph(sub_graph_def,\n",
    "                                  \"./mnist_cnn_small_dropout08_lr04\",\n",
    "                                  \"mnist_cnn.pb\",\n",
    "                                  as_text=False)\n",
    "\n",
    "print('written graph to: %s' % graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(y_pred, feed_dict={x: mnist.test.images[[3, 2, 1, 18, 4, 15, 11, 0, 61, 7], :].reshape(10, 28, 28, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
