// Auto generated by utensor-cli

#include "uTensor/ops/MatrixOps.hpp"
#include "mnist_cnn_weight_noprune_weight.hpp"
#include "mnist_cnn_weight_noprune.hpp"
#include "uTensor/ops/MathOps.hpp"
#include "uTensor/core/tensor.hpp"
#include "uTensor/ops/ArrayOps.hpp"
#include "uTensor/ops/NnOps.hpp"
#include "uTensor/core/context.hpp"


void get_mnist_cnn_weight_noprune_ctx(Context& ctx, Tensor* input_0) {

{ // add tensor for placeholders
    ctx.add(input_0, "x:0", 2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_conv_map_eightbit_x__port__0_reshape_dims_0), 
            "conv1/conv_map_eightbit/x__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv1/conv_map_eightbit/x__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "x:0", "conv1/conv_map_eightbit/x__port__0/reshape_dims:0" },
             { "conv1/conv_map_eightbit/x__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_conv_map_eightbit_x__port__0_reduction_dims_0), 
            "conv1/conv_map_eightbit/x__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/conv_map_eightbit/x__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv1/conv_map_eightbit/x__port__0/reshape:0", "conv1/conv_map_eightbit/x__port__0/reduction_dims:0" },
             { "conv1/conv_map_eightbit/x__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/conv_map_eightbit/x__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv1/conv_map_eightbit/x__port__0/reshape:0", "conv1/conv_map_eightbit/x__port__0/reduction_dims:0" },
             { "conv1/conv_map_eightbit/x__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv1/conv_map_eightbit/x__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map_eightbit/x__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map_eightbit/x__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "x:0",  "conv1/conv_map_eightbit/x__port__0/min:0", "conv1/conv_map_eightbit/x__port__0/max:0" },
             {  "conv1/conv_map_eightbit/x__port__0/quantize:0",  "conv1/conv_map_eightbit/x__port__0/quantize:1", "conv1/conv_map_eightbit/x__port__0/quantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({5,5,1,16}, inline_conv1_weights_0), 
            "conv1/weights:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_conv_map_eightbit_conv1_weights__port__0_reshape_dims_0), 
            "conv1/conv_map_eightbit/conv1/weights__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv1/conv_map_eightbit/conv1/weights__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv1/weights:0", "conv1/conv_map_eightbit/conv1/weights__port__0/reshape_dims:0" },
             { "conv1/conv_map_eightbit/conv1/weights__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_conv_map_eightbit_conv1_weights__port__0_reduction_dims_0), 
            "conv1/conv_map_eightbit/conv1/weights__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/conv_map_eightbit/conv1/weights__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv1/conv_map_eightbit/conv1/weights__port__0/reshape:0", "conv1/conv_map_eightbit/conv1/weights__port__0/reduction_dims:0" },
             { "conv1/conv_map_eightbit/conv1/weights__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/conv_map_eightbit/conv1/weights__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv1/conv_map_eightbit/conv1/weights__port__0/reshape:0", "conv1/conv_map_eightbit/conv1/weights__port__0/reduction_dims:0" },
             { "conv1/conv_map_eightbit/conv1/weights__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv1/weights:0",  "conv1/conv_map_eightbit/conv1/weights__port__0/min:0", "conv1/conv_map_eightbit/conv1/weights__port__0/max:0" },
             {  "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:0",  "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:1", "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv1/conv_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, SAME), 
             { "conv1/conv_map_eightbit/x__port__0/quantize:0", "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:0", "conv1/conv_map_eightbit/x__port__0/quantize:1", "conv1/conv_map_eightbit/x__port__0/quantize:2", "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:1", "conv1/conv_map_eightbit/conv1/weights__port__0/quantize:2" },
             { "conv1/conv_map/eightbit:0", "conv1/conv_map/eightbit:1", "conv1/conv_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv1/conv_map/eightbit:0", "conv1/conv_map/eightbit:1", "conv1/conv_map/eightbit:2" },
             { "conv1/conv_map/eightbit/requant_range:0", "conv1/conv_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv1/conv_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/conv_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv1/conv_map/eightbit:0", "conv1/conv_map/eightbit:1", "conv1/conv_map/eightbit:2", "conv1/conv_map/eightbit/requant_range:0", "conv1/conv_map/eightbit/requant_range:1" },
             { "conv1/conv_map/eightbit/requantize:0", "conv1/conv_map/eightbit/requantize:1", "conv1/conv_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({16}, inline_conv1_bias_0), 
            "conv1/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_Add_eightbit_conv1_bias__port__0_reshape_dims_0), 
            "conv1/Add_eightbit/conv1/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv1/Add_eightbit/conv1/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv1/bias:0", "conv1/Add_eightbit/conv1/bias__port__0/reshape_dims:0" },
             { "conv1/Add_eightbit/conv1/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv1_Add_eightbit_conv1_bias__port__0_reduction_dims_0), 
            "conv1/Add_eightbit/conv1/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/Add_eightbit/conv1/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv1/Add_eightbit/conv1/bias__port__0/reshape:0", "conv1/Add_eightbit/conv1/bias__port__0/reduction_dims:0" },
             { "conv1/Add_eightbit/conv1/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv1/Add_eightbit/conv1/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv1/Add_eightbit/conv1/bias__port__0/reshape:0", "conv1/Add_eightbit/conv1/bias__port__0/reduction_dims:0" },
             { "conv1/Add_eightbit/conv1/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv1/Add_eightbit/conv1/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/Add_eightbit/conv1/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/Add_eightbit/conv1/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv1/bias:0",  "conv1/Add_eightbit/conv1/bias__port__0/min:0", "conv1/Add_eightbit/conv1/bias__port__0/max:0" },
             {  "conv1/Add_eightbit/conv1/bias__port__0/quantize:0",  "conv1/Add_eightbit/conv1/bias__port__0/quantize:1", "conv1/Add_eightbit/conv1/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv1/Add/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv1/conv_map/eightbit/requantize:0", "conv1/conv_map/eightbit/requantize:1", "conv1/conv_map/eightbit/requantize:2", "conv1/Add_eightbit/conv1/bias__port__0/quantize:0", "conv1/Add_eightbit/conv1/bias__port__0/quantize:1",  "conv1/Add_eightbit/conv1/bias__port__0/quantize:2" },
             { "conv1/Add/eightbit:0", "conv1/Add/eightbit:1",  "conv1/Add/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv1/Add/eightbit:0", "conv1/Add/eightbit:1", "conv1/Add/eightbit:2" },
             { "conv1/Add/eightbit/requant_range:0", "conv1/Add/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv1/Add/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/Add/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv1/Add/eightbit:0", "conv1/Add/eightbit:1", "conv1/Add/eightbit:2", "conv1/Add/eightbit/requant_range:0", "conv1/Add/eightbit/requant_range:1" },
             { "conv1/Add/eightbit/requantize:0", "conv1/Add/eightbit/requantize:1", "conv1/Add/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv1/activation/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/activation/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv1/activation/eightbit:2", 1);
    ctx.push(new ReluOp<uint8_t, float, uint8_t>(), 
             { "conv1/Add/eightbit/requantize:0", "conv1/Add/eightbit/requantize:1", "conv1/Add/eightbit/requantize:2" },
             { "conv1/activation/eightbit:0", "conv1/activation/eightbit:1", "conv1/activation/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "max_pool1/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "max_pool1/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "max_pool1/eightbit:2", 1);

    ctx.push(new QuantizedMaxPoolingOp<uint8_t>(2, 2, 2, 2, SAME),
             { "conv1/activation/eightbit:0", "conv1/activation/eightbit:1", "conv1/activation/eightbit:2" }, 
             { "max_pool1/eightbit:0", "max_pool1/eightbit:1",  "max_pool1/eightbit:2" });

    
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({5,5,16,32}, inline_conv2_weights_quantized_const_0), 
            "conv2/weights_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv2_weights_quantized_min_0), 
            "conv2/weights_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv2_weights_quantized_max_0), 
            "conv2/weights_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv2/conv_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, SAME), 
             { "max_pool1/eightbit:0", "conv2/weights_quantized_const:0", "max_pool1/eightbit:1", "max_pool1/eightbit:2", "conv2/weights_quantized_min:0", "conv2/weights_quantized_max:0" },
             { "conv2/conv_map/eightbit:0", "conv2/conv_map/eightbit:1", "conv2/conv_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv2/conv_map/eightbit:0", "conv2/conv_map/eightbit:1", "conv2/conv_map/eightbit:2" },
             { "conv2/conv_map/eightbit/requant_range:0", "conv2/conv_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv2/conv_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/conv_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv2/conv_map/eightbit:0", "conv2/conv_map/eightbit:1", "conv2/conv_map/eightbit:2", "conv2/conv_map/eightbit/requant_range:0", "conv2/conv_map/eightbit/requant_range:1" },
             { "conv2/conv_map/eightbit/requantize:0", "conv2/conv_map/eightbit/requantize:1", "conv2/conv_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({32}, inline_conv2_bias_0), 
            "conv2/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv2_Add_eightbit_conv2_bias__port__0_reshape_dims_0), 
            "conv2/Add_eightbit/conv2/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv2/Add_eightbit/conv2/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv2/bias:0", "conv2/Add_eightbit/conv2/bias__port__0/reshape_dims:0" },
             { "conv2/Add_eightbit/conv2/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv2_Add_eightbit_conv2_bias__port__0_reduction_dims_0), 
            "conv2/Add_eightbit/conv2/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv2/Add_eightbit/conv2/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv2/Add_eightbit/conv2/bias__port__0/reshape:0", "conv2/Add_eightbit/conv2/bias__port__0/reduction_dims:0" },
             { "conv2/Add_eightbit/conv2/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv2/Add_eightbit/conv2/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv2/Add_eightbit/conv2/bias__port__0/reshape:0", "conv2/Add_eightbit/conv2/bias__port__0/reduction_dims:0" },
             { "conv2/Add_eightbit/conv2/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv2/Add_eightbit/conv2/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/Add_eightbit/conv2/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/Add_eightbit/conv2/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv2/bias:0",  "conv2/Add_eightbit/conv2/bias__port__0/min:0", "conv2/Add_eightbit/conv2/bias__port__0/max:0" },
             {  "conv2/Add_eightbit/conv2/bias__port__0/quantize:0",  "conv2/Add_eightbit/conv2/bias__port__0/quantize:1", "conv2/Add_eightbit/conv2/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv2/Add/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv2/conv_map/eightbit/requantize:0", "conv2/conv_map/eightbit/requantize:1", "conv2/conv_map/eightbit/requantize:2", "conv2/Add_eightbit/conv2/bias__port__0/quantize:0", "conv2/Add_eightbit/conv2/bias__port__0/quantize:1",  "conv2/Add_eightbit/conv2/bias__port__0/quantize:2" },
             { "conv2/Add/eightbit:0", "conv2/Add/eightbit:1",  "conv2/Add/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv2/Add/eightbit:0", "conv2/Add/eightbit:1", "conv2/Add/eightbit:2" },
             { "conv2/Add/eightbit/requant_range:0", "conv2/Add/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv2/Add/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/Add/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv2/Add/eightbit:0", "conv2/Add/eightbit:1", "conv2/Add/eightbit:2", "conv2/Add/eightbit/requant_range:0", "conv2/Add/eightbit/requant_range:1" },
             { "conv2/Add/eightbit/requantize:0", "conv2/Add/eightbit/requantize:1", "conv2/Add/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv2/relu/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/relu/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv2/relu/eightbit:2", 1);
    ctx.push(new ReluOp<uint8_t, float, uint8_t>(), 
             { "conv2/Add/eightbit/requantize:0", "conv2/Add/eightbit/requantize:1", "conv2/Add/eightbit/requantize:2" },
             { "conv2/relu/eightbit:0", "conv2/relu/eightbit:1", "conv2/relu/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "max_pooling2/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "max_pooling2/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "max_pooling2/eightbit:2", 1);

    ctx.push(new QuantizedMaxPoolingOp<uint8_t>(2, 2, 2, 2, SAME),
             { "conv2/relu/eightbit:0", "conv2/relu/eightbit:1", "conv2/relu/eightbit:2" }, 
             { "max_pooling2/eightbit:0", "max_pooling2/eightbit:1",  "max_pooling2/eightbit:2" });

    
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({5,5,32,64}, inline_conv3_weights_quantized_const_0), 
            "conv3/weights_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv3_weights_quantized_min_0), 
            "conv3/weights_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_conv3_weights_quantized_max_0), 
            "conv3/weights_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "conv3/conv_map/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit:2", 2);
    ctx.push(new QntConvOp<uint8_t, uint8_t, int>({ 1, 1, 1, 1 }, SAME), 
             { "max_pooling2/eightbit:0", "conv3/weights_quantized_const:0", "max_pooling2/eightbit:1", "max_pooling2/eightbit:2", "conv3/weights_quantized_min:0", "conv3/weights_quantized_max:0" },
             { "conv3/conv_map/eightbit:0", "conv3/conv_map/eightbit:1", "conv3/conv_map/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv3/conv_map/eightbit:0", "conv3/conv_map/eightbit:1", "conv3/conv_map/eightbit:2" },
             { "conv3/conv_map/eightbit/requant_range:0", "conv3/conv_map/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv3/conv_map/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/conv_map/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv3/conv_map/eightbit:0", "conv3/conv_map/eightbit:1", "conv3/conv_map/eightbit:2", "conv3/conv_map/eightbit/requant_range:0", "conv3/conv_map/eightbit/requant_range:1" },
             { "conv3/conv_map/eightbit/requantize:0", "conv3/conv_map/eightbit/requantize:1", "conv3/conv_map/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({64}, inline_conv3_bias_0), 
            "conv3/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv3_Add_eightbit_conv3_bias__port__0_reshape_dims_0), 
            "conv3/Add_eightbit/conv3/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "conv3/Add_eightbit/conv3/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "conv3/bias:0", "conv3/Add_eightbit/conv3/bias__port__0/reshape_dims:0" },
             { "conv3/Add_eightbit/conv3/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_conv3_Add_eightbit_conv3_bias__port__0_reduction_dims_0), 
            "conv3/Add_eightbit/conv3/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv3/Add_eightbit/conv3/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "conv3/Add_eightbit/conv3/bias__port__0/reshape:0", "conv3/Add_eightbit/conv3/bias__port__0/reduction_dims:0" },
             { "conv3/Add_eightbit/conv3/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "conv3/Add_eightbit/conv3/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "conv3/Add_eightbit/conv3/bias__port__0/reshape:0", "conv3/Add_eightbit/conv3/bias__port__0/reduction_dims:0" },
             { "conv3/Add_eightbit/conv3/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv3/Add_eightbit/conv3/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/Add_eightbit/conv3/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/Add_eightbit/conv3/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "conv3/bias:0",  "conv3/Add_eightbit/conv3/bias__port__0/min:0", "conv3/Add_eightbit/conv3/bias__port__0/max:0" },
             {  "conv3/Add_eightbit/conv3/bias__port__0/quantize:0",  "conv3/Add_eightbit/conv3/bias__port__0/quantize:1", "conv3/Add_eightbit/conv3/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "conv3/Add/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "conv3/conv_map/eightbit/requantize:0", "conv3/conv_map/eightbit/requantize:1", "conv3/conv_map/eightbit/requantize:2", "conv3/Add_eightbit/conv3/bias__port__0/quantize:0", "conv3/Add_eightbit/conv3/bias__port__0/quantize:1",  "conv3/Add_eightbit/conv3/bias__port__0/quantize:2" },
             { "conv3/Add/eightbit:0", "conv3/Add/eightbit:1",  "conv3/Add/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "conv3/Add/eightbit:0", "conv3/Add/eightbit:1", "conv3/Add/eightbit:2" },
             { "conv3/Add/eightbit/requant_range:0", "conv3/Add/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "conv3/Add/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/Add/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "conv3/Add/eightbit:0", "conv3/Add/eightbit:1", "conv3/Add/eightbit:2", "conv3/Add/eightbit/requant_range:0", "conv3/Add/eightbit/requant_range:1" },
             { "conv3/Add/eightbit/requantize:0", "conv3/Add/eightbit/requantize:1", "conv3/Add/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "conv3/relu/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/relu/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "conv3/relu/eightbit:2", 1);
    ctx.push(new ReluOp<uint8_t, float, uint8_t>(), 
             { "conv3/Add/eightbit/requantize:0", "conv3/Add/eightbit/requantize:1", "conv3/Add/eightbit/requantize:2" },
             { "conv3/relu/eightbit:0", "conv3/relu/eightbit:1", "conv3/relu/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "max_pooling3/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "max_pooling3/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "max_pooling3/eightbit:2", 1);

    ctx.push(new QuantizedMaxPoolingOp<uint8_t>(2, 2, 2, 2, SAME),
             { "conv3/relu/eightbit:0", "conv3/relu/eightbit:1", "conv3/relu/eightbit:2" }, 
             { "max_pooling3/eightbit:0", "max_pooling3/eightbit:1",  "max_pooling3/eightbit:2" });

    
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({2}, inline_flatten_shape_0), 
            "flatten/shape:0", 
            1);
}
{
    ctx.add(new RamTensor<uint8_t>(), "flatten/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "flatten/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "flatten/eightbit:2", 1);
    ctx.push(new QuantizedReshapeOp(),
              { "max_pooling3/eightbit:0", "flatten/shape:0", "max_pooling3/eightbit:1", "max_pooling3/eightbit:2" },
              { "flatten/eightbit:0", "flatten/eightbit:1", "flatten/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({1024,128}, inline_fc1_weights_quantized_const_0), 
            "fc1/weights_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fc1_weights_quantized_min_0), 
            "fc1/weights_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fc1_weights_quantized_max_0), 
            "fc1/weights_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "fc1/matmul/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit:2", 2);
    ctx.push(new QntMatMulOp<uint8_t, uint8_t, int>(), 
             { "flatten/eightbit:0", "flatten/eightbit:1", "flatten/eightbit:2", "fc1/weights_quantized_const:0", "fc1/weights_quantized_min:0",  "fc1/weights_quantized_max:0" },
             { "fc1/matmul/eightbit:0", "fc1/matmul/eightbit:1",  "fc1/matmul/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fc1/matmul/eightbit:0", "fc1/matmul/eightbit:1", "fc1/matmul/eightbit:2" },
             { "fc1/matmul/eightbit/requant_range:0", "fc1/matmul/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fc1/matmul/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fc1/matmul/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fc1/matmul/eightbit:0", "fc1/matmul/eightbit:1", "fc1/matmul/eightbit:2", "fc1/matmul/eightbit/requant_range:0", "fc1/matmul/eightbit/requant_range:1" },
             { "fc1/matmul/eightbit/requantize:0", "fc1/matmul/eightbit/requantize:1", "fc1/matmul/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fc1/relu/eightbit:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc1/relu/eightbit:1", 1);
    ctx.add(new RamTensor<float>({1}), "fc1/relu/eightbit:2", 1);
    ctx.push(new ReluOp<uint8_t, float, uint8_t>(), 
             { "fc1/matmul/eightbit/requantize:0", "fc1/matmul/eightbit/requantize:1", "fc1/matmul/eightbit/requantize:2" },
             { "fc1/relu/eightbit:0", "fc1/relu/eightbit:1", "fc1/relu/eightbit:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<uint8_t>({128,10}, inline_fc2_weights_quantized_const_0), 
            "fc2/weights_quantized_const:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fc2_weights_quantized_min_0), 
            "fc2/weights_quantized_min:0", 
            1);
}
{    
    ctx.add(new BinaryTensor<float>({1}, inline_fc2_weights_quantized_max_0), 
            "fc2/weights_quantized_max:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "fc2/matmul/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit:2", 2);
    ctx.push(new QntMatMulOp<uint8_t, uint8_t, int>(), 
             { "fc1/relu/eightbit:0", "fc1/relu/eightbit:1", "fc1/relu/eightbit:2", "fc2/weights_quantized_const:0", "fc2/weights_quantized_min:0",  "fc2/weights_quantized_max:0" },
             { "fc2/matmul/eightbit:0", "fc2/matmul/eightbit:1",  "fc2/matmul/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fc2/matmul/eightbit:0", "fc2/matmul/eightbit:1", "fc2/matmul/eightbit:2" },
             { "fc2/matmul/eightbit/requant_range:0", "fc2/matmul/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fc2/matmul/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/matmul/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fc2/matmul/eightbit:0", "fc2/matmul/eightbit:1", "fc2/matmul/eightbit:2", "fc2/matmul/eightbit/requant_range:0", "fc2/matmul/eightbit/requant_range:1" },
             { "fc2/matmul/eightbit/requantize:0", "fc2/matmul/eightbit/requantize:1", "fc2/matmul/eightbit/requantize:2" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<float>({10}, inline_fc2_bias_0), 
            "fc2/bias:0", 
            2);
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fc2_logits_eightbit_fc2_bias__port__0_reshape_dims_0), 
            "fc2/logits_eightbit/fc2/bias__port__0/reshape_dims:0", 
            1);
}
{
    ctx.add(new RamTensor<float>(), "fc2/logits_eightbit/fc2/bias__port__0/reshape:0", 2);
    ctx.push(new ReshapeOp(), 
             { "fc2/bias:0", "fc2/logits_eightbit/fc2/bias__port__0/reshape_dims:0" },
             { "fc2/logits_eightbit/fc2/bias__port__0/reshape:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fc2_logits_eightbit_fc2_bias__port__0_reduction_dims_0), 
            "fc2/logits_eightbit/fc2/bias__port__0/reduction_dims:0", 
            2);
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fc2/logits_eightbit/fc2/bias__port__0/min:0", 1);
    ctx.push(new MinOp(), 
             { "fc2/logits_eightbit/fc2/bias__port__0/reshape:0", "fc2/logits_eightbit/fc2/bias__port__0/reduction_dims:0" },
             { "fc2/logits_eightbit/fc2/bias__port__0/min:0" });
    ctx.eval();
}
{   
    RamTensor<float>* out_tensor;
    out_tensor = new RamTensor<float>({ 1 });
    ctx.add(out_tensor, "fc2/logits_eightbit/fc2/bias__port__0/max:0", 1);
    ctx.push(new MaxOp(), 
             { "fc2/logits_eightbit/fc2/bias__port__0/reshape:0", "fc2/logits_eightbit/fc2/bias__port__0/reduction_dims:0" },
             { "fc2/logits_eightbit/fc2/bias__port__0/max:0" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<uint8_t>(), "fc2/logits_eightbit/fc2/bias__port__0/quantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/logits_eightbit/fc2/bias__port__0/quantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/logits_eightbit/fc2/bias__port__0/quantize:2", 1);
    ctx.push(new QuantizeV2Op(),
             {  "fc2/bias:0",  "fc2/logits_eightbit/fc2/bias__port__0/min:0", "fc2/logits_eightbit/fc2/bias__port__0/max:0" },
             {  "fc2/logits_eightbit/fc2/bias__port__0/quantize:0",  "fc2/logits_eightbit/fc2/bias__port__0/quantize:1", "fc2/logits_eightbit/fc2/bias__port__0/quantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<int>(), "fc2/logits/eightbit:0", 2);
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit:1", 2);
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit:2", 2);
    ctx.push(new QuantizedAddOp<uint8_t, uint8_t, int>(), 
             { "fc2/matmul/eightbit/requantize:0", "fc2/matmul/eightbit/requantize:1", "fc2/matmul/eightbit/requantize:2", "fc2/logits_eightbit/fc2/bias__port__0/quantize:0", "fc2/logits_eightbit/fc2/bias__port__0/quantize:1",  "fc2/logits_eightbit/fc2/bias__port__0/quantize:2" },
             { "fc2/logits/eightbit:0", "fc2/logits/eightbit:1",  "fc2/logits/eightbit:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit/requant_range:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit/requant_range:1", 1);
    ctx.push(new Requantization_RangeOp(),
             { "fc2/logits/eightbit:0", "fc2/logits/eightbit:1", "fc2/logits/eightbit:2" },
             { "fc2/logits/eightbit/requant_range:0", "fc2/logits/eightbit/requant_range:1" });
    ctx.eval();
}
{   
    ctx.add(new RamTensor<uint8_t>(), "fc2/logits/eightbit/requantize:0", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit/requantize:1", 1);
    ctx.add(new RamTensor<float>({1}), "fc2/logits/eightbit/requantize:2", 1);
    ctx.push(new RequantizeOp(),
             { "fc2/logits/eightbit:0", "fc2/logits/eightbit:1", "fc2/logits/eightbit:2", "fc2/logits/eightbit/requant_range:0", "fc2/logits/eightbit/requant_range:1" },
             { "fc2/logits/eightbit/requantize:0", "fc2/logits/eightbit/requantize:1", "fc2/logits/eightbit/requantize:2" });
    ctx.eval();
}
{
    ctx.add(new RamTensor<float>(), "fc2/logits:0", 1);
    ctx.push(new DequantizeOp(), 
             { "fc2/logits/eightbit/requantize:0", "fc2/logits/eightbit/requantize:1", "fc2/logits/eightbit/requantize:2" },
             { "fc2/logits:0" });
    ctx.eval();
}
{    
    ctx.add(new BinaryTensor<int>({1}, inline_fc2_y_pred_dimension_0), 
            "fc2/y_pred/dimension:0", 
            1);
}
{
    ctx.add(new RamTensor<int>(), "fc2/y_pred:0");
    ctx.push(new ArgMaxOp<float, int>(), 
             { "fc2/logits:0", "fc2/y_pred/dimension:0" },
             { "fc2/y_pred:0" });
}
}